{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Convolution2D.py\n"
     ]
    }
   ],
   "source": [
    "%%file Convolution2D.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def cv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "    def convolution2d(input_m):\n",
    "        if bias:\n",
    "            value_b = torch.rand(out_channels)\n",
    "        else:\n",
    "            value_b = torch.zeros(out_channels)\n",
    "\n",
    "        # Проверяем правила для свертки с группами\n",
    "        assert in_channels % groups == 0\n",
    "        assert out_channels % groups == 0\n",
    "\n",
    "        if padding_mode == 'zeros':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "        elif padding_mode == 'reflect':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='reflect')\n",
    "        elif padding_mode == 'replicate':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='replicate')\n",
    "        elif padding_mode == 'circular':\n",
    "            input_m = circular_pad(input_m, padding)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported padding_mode\")\n",
    "\n",
    "        if type(kernel_size) == tuple:\n",
    "            filter = torch.rand(out_channels, in_channels // groups, kernel_size[0], kernel_size[1])\n",
    "        elif type(kernel_size) == int:\n",
    "            filter = torch.rand(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel_size type\")\n",
    "\n",
    "        out_tensor = []\n",
    "        for l in range(out_channels):\n",
    "            f = np.array([])\n",
    "            for i in range (0, input_m.shape[1] - ((filter.shape[2]-1) * dilation + 1) + 1, stride):\n",
    "                for j in range (0, input_m.shape[2] - ((filter.shape[3]-1) * dilation + 1) + 1, stride):\n",
    "                    s = 0\n",
    "                    for c in range (in_channels//groups):\n",
    "                        if groups > 1:\n",
    "                            val = input_m[l * (in_channels//groups) + c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3]-1) * dilation + 1:dilation]\n",
    "                        else:\n",
    "                            val = input_m[c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3] - 1) * dilation + 1:dilation]\n",
    "                        mini_sum = (val * filter[l][c]).sum()\n",
    "                        s = s + mini_sum\n",
    "                    f = np.append(f, float(s + value_b[l]))\n",
    "            out_tensor.append(torch.tensor(f, dtype=torch.float).view(1, 1, -1))\n",
    "        return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
    "    return convolution2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_convolution2d.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_convolution2d.py\n",
    "import torch\n",
    "import pytest\n",
    "from Convolution2D import cv2d\n",
    "import torch.nn as nn\n",
    "\n",
    "def test_cv2d_1():\n",
    "    tensor = torch.rand(8, 5, 6)\n",
    "\n",
    "    Convolution2D = cv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=0, dilation=1, groups=4, bias=True, padding_mode='zeros')\n",
    "    result, kernel_size, bias = Convolution2D(tensor)\n",
    "    torchFunction = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=0, dilation=1, groups=4, bias=True, padding_mode='zeros')\n",
    "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "    torchFunction.bias.data = torch.tensor(bias)\n",
    "\n",
    "def test_cv2d_2():\n",
    "    tensor = torch.rand(4, 5, 5)\n",
    "\n",
    "    Convolution2D = cv2d(in_channels=4, out_channels=2, kernel_size=3, stride=2, padding=2, dilation=1, groups=2, bias=True, padding_mode='reflect')\n",
    "    result, kernel_size, bias = Convolution2D(tensor)\n",
    "    torchFunction = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=3, stride=2, padding=2, dilation=1, groups=2, bias=True, padding_mode='reflect')\n",
    "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "    torchFunction.bias.data = torch.tensor(bias)\n",
    "\n",
    "def test_cv2d_3():\n",
    "    tensor = torch.rand(2, 2, 2)\n",
    "\n",
    "    Convolution2D = cv2d(in_channels=2, out_channels=2, kernel_size=2, stride=2, padding=0, dilation=1, groups=2, bias=True, padding_mode='zeros')\n",
    "    result, kernel_size, bias = Convolution2D(tensor)\n",
    "    torchFunction = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=2, stride=2, padding=0, dilation=1, groups=2, bias=True, padding_mode='zeros')\n",
    "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "    torchFunction.bias.data = torch.tensor(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.9.16, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: d:\\SMZ\\lab1\n",
      "plugins: anyio-3.5.0\n",
      "collected 3 items\n",
      "\n",
      "test_convolution2d.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "test_convolution2d.py::test_cv2d_1\n",
      "test_convolution2d.py::test_cv2d_2\n",
      "test_convolution2d.py::test_cv2d_3\n",
      "  d:\\SMZ\\lab1\\Convolution2D.py:49: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "    return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
      "\n",
      "test_convolution2d.py::test_cv2d_1\n",
      "test_convolution2d.py::test_cv2d_2\n",
      "test_convolution2d.py::test_cv2d_3\n",
      "  d:\\SMZ\\lab1\\Convolution2D.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "    return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
      "\n",
      "test_convolution2d.py::test_cv2d_1\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "\n",
      "test_convolution2d.py::test_cv2d_1\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.bias.data = torch.tensor(bias)\n",
      "\n",
      "test_convolution2d.py::test_cv2d_2\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "\n",
      "test_convolution2d.py::test_cv2d_2\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.bias.data = torch.tensor(bias)\n",
      "\n",
      "test_convolution2d.py::test_cv2d_3\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "\n",
      "test_convolution2d.py::test_cv2d_3\n",
      "  d:\\SMZ\\lab1\\test_convolution2d.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "    torchFunction.bias.data = torch.tensor(bias)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m12 warnings\u001b[0m\u001b[33m in 1.28s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolution_2d(input_tensor, weight, bias, stride=1, padding=0):\n",
    "    # Получаем размеры входного тензора и ядра\n",
    "    batch_size, in_channels, in_height, in_width = input_tensor.shape\n",
    "    out_channels, _, kernel_size, _ = weight.shape\n",
    "\n",
    "    # Рассчитываем размеры выходного тензора\n",
    "    out_height = (in_height + 2 * padding - kernel_size) // stride + 1\n",
    "    out_width = (in_width + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "    # Создаем выходной тензор\n",
    "    output_tensor = np.zeros((batch_size, out_channels, out_height, out_width))\n",
    "\n",
    "    # Применяем свертку\n",
    "    for b in range(batch_size):\n",
    "        for oc in range(out_channels):\n",
    "            for oh in range(0, out_height * stride, stride):\n",
    "                for ow in range(0, out_width * stride, stride):\n",
    "                    # Выделяем кусок входного тензора\n",
    "                    input_slice = input_tensor[b, :, oh:oh+kernel_size, ow:ow+kernel_size]\n",
    "\n",
    "                    # Применяем свертку (поэлементное умножение и суммирование)\n",
    "                    output_tensor[b, oc, oh//stride, ow//stride] = np.sum(input_slice * weight[oc]) + bias[oc]\n",
    "\n",
    "    return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Output:\n",
      "[[[[6.20261047 6.8446192 ]\n",
      "   [5.75773456 6.86908592]]\n",
      "\n",
      "  [[6.55674849 6.82567574]\n",
      "   [6.60225017 6.1037361 ]]\n",
      "\n",
      "  [[6.02688642 6.26373563]\n",
      "   [6.05833308 6.05528529]]\n",
      "\n",
      "  [[5.87503785 5.14720024]\n",
      "   [4.97920085 5.27788051]]\n",
      "\n",
      "  [[5.9773733  6.21228235]\n",
      "   [5.77192805 5.66118583]]]\n",
      "\n",
      "\n",
      " [[[9.04392416 7.64726451]\n",
      "   [9.86783607 7.07144742]]\n",
      "\n",
      "  [[8.62346548 9.32910253]\n",
      "   [9.49971242 8.14498258]]\n",
      "\n",
      "  [[8.26695763 8.32120175]\n",
      "   [7.92308355 7.44420464]]\n",
      "\n",
      "  [[7.57249632 6.55402827]\n",
      "   [7.00368381 6.84683706]]\n",
      "\n",
      "  [[8.62039515 8.86813249]\n",
      "   [8.63951339 7.59392805]]]]\n",
      "\n",
      "PyTorch Output:\n",
      "[[[[ 0.37422693  0.6152117 ]\n",
      "   [ 0.6018531   0.6573878 ]]\n",
      "\n",
      "  [[ 0.248276   -0.07862993]\n",
      "   [ 0.2556469   0.02925629]]\n",
      "\n",
      "  [[ 0.5145069   0.09180786]\n",
      "   [ 0.47800478  0.21590856]]\n",
      "\n",
      "  [[-0.12584819 -0.026763  ]\n",
      "   [ 0.0746723  -0.1294945 ]]\n",
      "\n",
      "  [[ 0.11321617  0.26693416]\n",
      "   [ 0.04741885  0.56894374]]]\n",
      "\n",
      "\n",
      " [[[ 0.68607926  0.663958  ]\n",
      "   [ 0.6664135   0.64351314]]\n",
      "\n",
      "  [[ 0.47675046  0.02494059]\n",
      "   [ 0.2716239  -0.07753362]]\n",
      "\n",
      "  [[ 0.6611885   0.7112959 ]\n",
      "   [ 0.78615254  0.3628146 ]]\n",
      "\n",
      "  [[-0.34499705  0.02113049]\n",
      "   [-0.140962   -0.2631607 ]]\n",
      "\n",
      "  [[ 0.5497651   0.4282693 ]\n",
      "   [ 0.35657808  0.3295007 ]]]]\n",
      "\n",
      "Difference:\n",
      "[[[[5.82838354 6.22940747]\n",
      "   [5.15588149 6.21169813]]\n",
      "\n",
      "  [[6.3084725  6.90430568]\n",
      "   [6.34660326 6.0744798 ]]\n",
      "\n",
      "  [[5.51237955 6.17192778]\n",
      "   [5.5803283  5.83937674]]\n",
      "\n",
      "  [[6.00088604 5.17396325]\n",
      "   [4.90452855 5.40737502]]\n",
      "\n",
      "  [[5.86415713 5.9453482 ]\n",
      "   [5.7245092  5.09224209]]]\n",
      "\n",
      "\n",
      " [[[8.35784489 6.9833065 ]\n",
      "   [9.20142259 6.42793428]]\n",
      "\n",
      "  [[8.14671501 9.30416194]\n",
      "   [9.22808851 8.2225162 ]]\n",
      "\n",
      "  [[7.60576915 7.60990585]\n",
      "   [7.13693101 7.08139003]]\n",
      "\n",
      "  [[7.91749337 6.53289778]\n",
      "   [7.14464581 7.10999777]]\n",
      "\n",
      "  [[8.07063004 8.43986319]\n",
      "   [8.2829353  7.26442734]]]]\n"
     ]
    }
   ],
   "source": [
    "def test_convolution_2d():\n",
    "    # Создаем входной тензор, веса и смещение\n",
    "    input_tensor = np.random.rand(2, 3, 4, 4)\n",
    "    weight = np.random.rand(5, 3, 3, 3)\n",
    "    bias = np.random.rand(5)\n",
    "\n",
    "    # Вызываем нашу функцию свертки\n",
    "    custom_output = convolution_2d(input_tensor, weight, bias)\n",
    "\n",
    "    # Используем встроенную функцию PyTorch для свертки\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    torch_conv = nn.Conv2d(3, 5, kernel_size=3)\n",
    "    torch_output = torch_conv(torch.tensor(input_tensor, dtype=torch.float32))\n",
    "    print(\"Custom Output:\")\n",
    "    print(custom_output)\n",
    "    print(\"\\nPyTorch Output:\")\n",
    "    print(torch_output.detach().numpy())\n",
    "\n",
    "    print(\"\\nDifference:\")\n",
    "    print(custom_output - torch_output.detach().numpy())\n",
    "\n",
    "    # Проверяем, совпадают ли результаты\n",
    "    # np.testing.assert_allclose(custom_output, torch_output.detach().numpy(), rtol=1e-5)\n",
    "\n",
    "# Запускаем тесты\n",
    "test_convolution_2d()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\SMZ\\lab1\\lab1.ipynb Ячейка 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Transforms\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m _transform \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     A\u001b[39m.\u001b[39mResize(height \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m), \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ToTensorV2(), \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trans_image \u001b[39m=\u001b[39m _transform(image\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(image))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SMZ/lab1/lab1.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(trans_image[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transforms\n",
    "_transform = A.Compose([\n",
    "    A.Resize(height = 512, width=512), \n",
    "    ToTensorV2(), \n",
    "])\n",
    "\n",
    "\n",
    "trans_image = _transform(image=np.array(image))\n",
    "outputs = model(trans_image['image'].float().unsqueeze(0))\n",
    "logits = outputs.logits.cpu()\n",
    "print(logits.shape)\n",
    "\n",
    "\n",
    "# First, rescale logits to original image size\n",
    "upsampled_logits = nn.functional.interpolate(logits,\n",
    "                size=image.size[::-1], # (height, width)\n",
    "                mode='bilinear',\n",
    "                align_corners=False)\n",
    "\n",
    "\n",
    "seg = upsampled_logits.argmax(dim=1)[0]\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "palette = np.array([[0, 0, 0],[255, 255, 255]])\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
