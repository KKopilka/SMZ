{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Conv_transpose2d.py\n"
     ]
    }
   ],
   "source": [
    "%%file Conv_transpose2d.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def conv_transpose2d(in_channels, out_channels, kernel_size, stride=1, groups=1, padding=0, output_padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
    "    def convolution_transpose2d(input_m):\n",
    "        if bias:\n",
    "            value_b = torch.rand(out_channels)\n",
    "        else:\n",
    "            value_b = torch.zeros(out_channels)\n",
    "\n",
    "        # Проверяем правила для свертки с группами\n",
    "        assert in_channels % groups == 0\n",
    "        assert out_channels % groups == 0\n",
    "\n",
    "        if padding_mode == 'reflect' or padding_mode == 'replicate' or padding_mode == 'circular':\n",
    "            raise ValueError(\"Unsupported padding_mode\")\n",
    "\n",
    "        if type(kernel_size) == tuple:\n",
    "            filter = torch.rand(in_channels, out_channels, kernel_size[0], kernel_size[1])\n",
    "        elif type(kernel_size) == int:\n",
    "            filter = torch.rand(in_channels, out_channels, kernel_size, kernel_size)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel_size type\")\n",
    "\n",
    "        out_tensor = []\n",
    "        for l in range(out_channels):\n",
    "            f = np.array([])\n",
    "            for i in range (0, input_m.shape[1] - ((filter.shape[2]-1) * dilation + 1) + 1, stride):\n",
    "                for j in range (0, input_m.shape[2] - ((filter.shape[3]-1) * dilation + 1) + 1, stride):\n",
    "                    s = 0\n",
    "                    for c in range (in_channels//groups):\n",
    "                        if groups > 1:\n",
    "                            val = input_m[l * (in_channels//groups) + c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3]-1) * dilation + 1:dilation]\n",
    "                        else:\n",
    "                            val = input_m[c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3] - 1) * dilation + 1:dilation]\n",
    "                        mini_sum = (val * filter[l][c]).sum()\n",
    "                        s = s + mini_sum\n",
    "                    f = np.append(f, float(s + value_b[l]))\n",
    "            out_tensor.append(torch.tensor(f, dtype=torch.float).view(1, 1, -1))\n",
    "        return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
    "    return convolution_transpose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_conv_transpose2d.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_conv_transpose2d.py\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Conv_transpose2d import conv_transpose2d\n",
    "\n",
    "class TestConvolutionTranspose2D(unittest.TestCase):\n",
    "    def test_conv_transpose2d_1(self):\n",
    "        tensor = torch.rand(2, 10, 10)\n",
    "\n",
    "        ConvTranspose2D = conv_transpose2d(in_channels=2, out_channels=2, kernel_size=2, stride=10, padding=0, output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "        result, kernel_size, bias = ConvTranspose2D(tensor)\n",
    "        \n",
    "        torchFunction = nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=2, stride=10, padding=0, output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "        torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "        torchFunction.bias.data = torch.tensor(bias)\n",
    "\n",
    "    def test_conv_transpose2d_2(self):\n",
    "        tensor = torch.rand(1, 1, 1)\n",
    "\n",
    "        ConvTranspose2D = conv_transpose2d(in_channels=1, out_channels=1, kernel_size=1, stride=1, padding=0, output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "        result, kernel_size, bias = ConvTranspose2D(tensor)\n",
    "        \n",
    "        torchFunction = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=1, stride=1, padding=0, output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "        torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "        torchFunction.bias.data = torch.tensor(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SMZ\\lab3\\Conv_transpose2d.py:41: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
      "d:\\SMZ\\lab3\\Conv_transpose2d.py:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(out_tensor), torch.tensor(np.array(filter)), torch.tensor(np.array(value_b))\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.bias.data = torch.tensor(bias)\n",
      ".d:\\SMZ\\lab3\\test_conv_transpose2d.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.bias.data = torch.tensor(bias)\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_conv_transpose2d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Convolution2D_dop.py\n"
     ]
    }
   ],
   "source": [
    "%%file Convolution2D_dop.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def cv2d_dop(in_channels, out_channels, kernel_size, transp_stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "    def convolution2d_dop(input_m):\n",
    "        if bias:\n",
    "            value_b = torch.rand(out_channels)\n",
    "        else:\n",
    "            value_b = torch.zeros(out_channels)\n",
    "\n",
    "        # Проверяем правила для свертки с группами\n",
    "        assert in_channels % groups == 0\n",
    "        assert out_channels % groups == 0\n",
    "\n",
    "        if padding_mode == 'zeros':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='constant', value=0)\n",
    "        elif padding_mode == 'reflect':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='reflect')\n",
    "        elif padding_mode == 'replicate':\n",
    "            input_m = F.pad(input_m, (padding, padding, padding, padding), mode='replicate')\n",
    "        elif padding_mode == 'circular':\n",
    "            input_m = circular_pad(input_m, padding)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported padding_mode\")\n",
    "\n",
    "        if type(kernel_size) == tuple:\n",
    "            filter = torch.rand(out_channels, in_channels // groups, kernel_size[0], kernel_size[1])\n",
    "        elif type(kernel_size) == int:\n",
    "            filter = torch.rand(out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel_size type\")\n",
    "        \n",
    "        stride = 1\n",
    "        result_matrix = []\n",
    "        for matr in input_m:\n",
    "            # Увеличиваем выборку входной матрицы с помощью transp_stride\n",
    "            upsampled_matr = np.kron(matr, np.ones((transp_stride, transp_stride)))\n",
    "            # Дополняем матрицу с увеличенной дискретизацией\n",
    "            pad = kernel_size - 1\n",
    "            pad_matr = np.pad(upsampled_matr, pad_width=pad, mode='constant')\n",
    "            result_matrix.append(pad_matr)\n",
    "        input_m = torch.tensor(result_matrix, dtype=torch.float)\n",
    "        \n",
    "        filter_np = np.array(torch.rand(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        filter_tensor = torch.tensor(filter_np)\n",
    "        filter_for_transpose = torch.flip(filter_tensor, dims=[2, 3]).permute(1, 0, 2, 3)\n",
    "\n",
    "        out_tensor = []\n",
    "        for l in range(out_channels):\n",
    "            f = np.array([])\n",
    "            for i in range(0, input_m.shape[1] - ((filter.shape[2]-1) * dilation + 1) + 1, stride):\n",
    "                for j in range(0, input_m.shape[2] - ((filter.shape[3]-1) * dilation + 1) + 1, stride):\n",
    "                    s = 0\n",
    "                    for c in range(in_channels // groups):\n",
    "                        if groups > 1:\n",
    "                            val = input_m[l * (in_channels // groups) + c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3]-1) * dilation + 1:dilation]\n",
    "                        else:\n",
    "                            val = input_m[c][i:i + (filter.shape[2]-1) * dilation + 1:dilation, j:j + (filter.shape[3] - 1) * dilation + 1:dilation]\n",
    "                        mini_sum = (val * filter[l][c]).sum()\n",
    "                        s = s + mini_sum\n",
    "                    f = np.append(f, float(s + value_b[l]))\n",
    "            out_tensor.append(torch.tensor(f, dtype=torch.float).view(1, 1, -1))\n",
    "\n",
    "        out_tensor_np = np.array([tensor.numpy() for tensor in out_tensor])\n",
    "        return out_tensor_np, filter_for_transpose, torch.tensor(np.array(value_b))\n",
    "\n",
    "    return convolution2d_dop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_conv_transpose2d_dop.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_conv_transpose2d_dop.py\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Convolution2D_dop import cv2d_dop\n",
    "\n",
    "class TestConvolutionTranspose2D_dop(unittest.TestCase):\n",
    "    def test_conv_transpose2d_dop_1(self):\n",
    "        tensor = torch.rand(2, 10, 10)\n",
    "\n",
    "        ConvTranspose2D = cv2d_dop(in_channels=2, out_channels=2, kernel_size=2, transp_stride=10, bias=True)\n",
    "        result, kernel_size, bias = ConvTranspose2D(tensor)\n",
    "        \n",
    "        torchFunction = nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=2, stride=10, bias=True)\n",
    "        torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "        torchFunction.bias.data = torch.tensor(bias)\n",
    "\n",
    "    def test_conv_transpose2d_dop_2(self):\n",
    "        tensor = torch.rand(1, 1, 1)\n",
    "\n",
    "        ConvTranspose2D = cv2d_dop(in_channels=1, out_channels=1, kernel_size=1, transp_stride=1, bias=True)\n",
    "        result, kernel_size, bias = ConvTranspose2D(tensor)\n",
    "        \n",
    "        torchFunction = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=1, stride=1, bias=True)\n",
    "        torchFunction.weight.data = torch.tensor(kernel_size)\n",
    "        torchFunction.bias.data = torch.tensor(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SMZ\\lab3\\Convolution2D_dop.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  input_m = torch.tensor(result_matrix, dtype=torch.float)\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d_dop.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d_dop.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.bias.data = torch.tensor(bias)\n",
      ".d:\\SMZ\\lab3\\test_conv_transpose2d_dop.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.weight.data = torch.tensor(kernel_size)\n",
      "d:\\SMZ\\lab3\\test_conv_transpose2d_dop.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torchFunction.bias.data = torch.tensor(bias)\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 2.953s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest test_conv_transpose2d_dop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
